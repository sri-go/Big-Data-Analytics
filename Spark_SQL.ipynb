{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark SQL",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sri-go/BigDataAnalytics/blob/master/Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qVMCjEV1IEC7"
      },
      "source": [
        "# CIS 545 Homework 3: Spark SQL\n",
        "\n",
        "Welcome to CIS 545 Homework 3! In this homework you will gain a mastery of using Spark SQL. By the end, you'll be a star (not that you aren't already one). Over the next few days you will be using an EMR cluster to use Spark to manipulate the entire `linkedin.json` dataset from Homework 2 as well as a new data set, `stock_prices.csv`.\n",
        "\n",
        "The goal of the homework will be to create a training dataset for a Random Forest Machine learning model. The training data set will contain the monthly number of employees hired by companies in `linkedin.json` and their corresponding closing stock prices over a 10 year period (2000-2011). We will try and predict, based on this data, if the company will have a positive or negative growth in stock in the first quarter of the next year. Who's ready to make some money?\n",
        "\n",
        "## The Noteworthy Notes\n",
        "Before we begin here are some important notes to keep in mind,\n",
        "\n",
        "\n",
        "1.   **IMPORTANT!** I said it twice, it's really important. In this homework, we will be using AWS resources. You are given a quota ($150) to use for the entirety of the homework. There is a small chance you will use all this money, however it is important that at the end of every session, you **shut down your EMR cluster**.\n",
        "2.   **You can only use Google Colab for this Homework** since we must connect to the EMR cluster and Jupyter doesn't do that. Using a Google Colab Notebook with an EMR cluster has two important abnormalities:\n",
        "    * The first line of any cell in which you will use the spark session must be `%%spark`. Notice that all cells below have this.\n",
        "    * You will, unfortunately, not be able to stop a cell while it is running. If you wish to do so, you will need to restart your cluster. See the Setup EMR Document for reference.\n",
        "3.   You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n",
        "4.   Throughout the homework you will be manipulating Spark dataframes (sdfs). We do not specify any ordering on the final output. You are welcome to order your final tables in whatever way you deem fit. We will conduct our own ordering when we grade.\n",
        "5. Speaking of grading, this homework introduces `The_Gallant_Grader` an astonishing autograder create by everybody's favorite TA, Leonardo Murri. After each work cell you will see a cell titled `## AUTOGRADER Step 1.X: Run this to get your score ##`. By running the cell you will get your score for a specific task. You are allowed to submit to the autograder as many times as you'd like. However, know that we can track all your submission so if you try to pull anything shady we'll figure it out.\n",
        "6. There are portions of this homework that are _very_ hard. We urge you start early to come to office hours (especially Fridays and Wednesdays) and get help if you get stuck. But don't worry, I can see the future, and you all got this.\n",
        "\n",
        "With that said, let's dive in.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7XEqGpEGBWs5"
      },
      "source": [
        "## Step 0: Set up EMR\n",
        "\n",
        "Your first task is to create an EMR cluster your AWS Educate Accounts. Please see the [attached document](https://drive.google.com/open?id=1_8NB_3QXfQKm5Vyu7XyxU2mnH5HnGM-F) for detailed instructions. Move on to Step 0.1 after you have completed all the steps in the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5iBPXxgAdXkv"
      },
      "source": [
        "### Step 0.1: The Superfluous Setup\n",
        "\n",
        "Run the following two cells. These will allow your colab notebook to connect to an use your EMR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "pvkEbVaaAQ1e",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev\n",
        "!pip install sparkmagic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "6WAJmQ8IAbRs",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CL6n768EPt9E"
      },
      "source": [
        "### Step 0.2: The Succulent Spark\n",
        "\n",
        "Now, connect your notebook to the EMR cluster you created. In the first cell, copy the link to the Master Public DNS specified in the setup document. You will need to add `http://` to the beginning of the address and the port to the end. The final format should be,\n",
        "\n",
        "`http://<your-DNS-link>:8998`\n",
        "\n",
        "For example, if my DNS (directly from the AWS EMR console) is `ec2-3-15-237-211.us-east-2.compute.amazonaws.com` my address would be,\n",
        "\n",
        "`http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com:8998`\n",
        "\n",
        "Insert this in the `# TODO # below`. For our example, the cell would read,\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "%spark add -s spark_session -l python -u http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com:8998\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9XWNkCEAeWP",
        "colab": {}
      },
      "source": [
        "# TODO: Enter your Master Public DNS with the proper formatting and host\n",
        "\n",
        "%spark add -s spark_session -l python -u http://ec2-54-166-207-105.compute-1.amazonaws.com:8998\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nf_ADEXnIK0b"
      },
      "source": [
        "## Step 1: Data Cleaning and Shaping\n",
        "\n",
        "The data you will use is stored in an S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). \n",
        "\n",
        "### Step 1.1: The Stupendous Schema\n",
        "\n",
        "When loading data, Spark will try to infer it's structure on it's own. This process is faulty because it will sometimes infer the type incorrectly. JSON documents, like the one we will use, can have nested types, such as: arrays, arrays of dictionaries, dictionaries of dictionaries, etc. Spark's ability to determine these nested types is not reliable, thus you will define a schema for `linkedin.json`.\n",
        "\n",
        "A schema is a description of the structure of data. You will be defining an explicit schema for `linkedin.json`. In Spark, schema's are defined using a `StructType` object. This is a collection of data types, termed `StructField`'s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object,\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        " \"student_name\": \"Leonardo Murri\",\n",
        " \"GPA\": 1.4,\n",
        " \"courses\": [\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 545\",\n",
        "     \"semester\": \"Fall 2018\"},\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 520\",\n",
        "     \"semester\": \"Fall 2018\"},\n",
        "    {\"department\": \"Electrical and Systems Engineering\",\n",
        "     \"course_id\": \"ESE 650\",\n",
        "     \"semester\": \"Spring 2018\"}\n",
        " ],\n",
        " \"grad_year\": 2019\n",
        " }\n",
        "```\n",
        "\n",
        "We would define it's schema as follows,\n",
        "\n",
        "```       \n",
        "schema = StructType([\n",
        "           StructField(\"student_name\", StringType(), nullable=True),\n",
        "           StructField(\"GPA\", FloatType(), nullable=True),\n",
        "           StructField(\"courses\", ArrayType(\n",
        "                StructType([\n",
        "                  StructField(\"department\", StringType(), nullable=True),\n",
        "                  StructField(\"course_id\", StringType(), nullable=True),\n",
        "                  StructField(\"semester\", StringType(), nullable=True)\n",
        "                ])\n",
        "           ), nullable=True),\n",
        "           StructField(\"grad_year\", IntegerType(), nullable=True)\n",
        "         ])\n",
        "```\n",
        "\n",
        "\n",
        "Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `linkedin.json`. A sample JSON object for the dataset can be found [here](http://oneclickpaste.com/194048/).\n",
        "\n",
        "_Note_: In `linkedin.json` the field `specilities` is spelled incorrectly. This is **not** a typo. There is also no grading cell for this step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pL-Ps4KWIJ9e",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Define [linkedin.json] schema\n",
        "\n",
        "schema =  StructType([\n",
        "          StructField(\"_id\", StringType(), True),\n",
        "          StructField(\"education\", ArrayType(\\\n",
        "              StructType([\n",
        "                StructField(\"major\", StringType(),nullable = True),\n",
        "                StructField(\"end\", StringType(), nullable = True),\n",
        "                StructField(\"name\", StringType(), nullable = True),\n",
        "                StructField(\"degree\", StringType(), nullable = True),\n",
        "                StructField(\"start\", StringType(), nullable = True),\n",
        "                StructField(\"desc\", StringType(), nullable = True)\n",
        "              ])), nullable=True),\n",
        "          StructField(\"group\", MapType(StringType(), ArrayType(StringType())), nullable = True),\n",
        "          StructField(\"name\", MapType(StringType(), StringType()), nullable = True),\n",
        "          StructField(\"locality\", StringType(), nullable = True),\n",
        "          StructField(\"skills\", ArrayType(StringType()), nullable = True),\n",
        "          StructField(\"industry\", StringType(), nullable = True),\n",
        "          StructField(\"interval\", IntegerType(), nullable = True),\n",
        "          StructField(\"experience\", ArrayType(\n",
        "              StructType([\\\n",
        "                  StructField(\"org\", StringType(), nullable = True),\n",
        "                  StructField(\"title\", StringType(), nullable = True),\n",
        "                  StructField(\"end\", StringType(), nullable = True),\n",
        "                  StructField(\"start\", StringType(), nullable = True),\n",
        "                  StructField(\"desc\", StringType(), nullable = True)\n",
        "              ])\n",
        "          ), nullable = True),\n",
        "          StructField(\"summary\", StringType(), nullable = True),\n",
        "          StructField(\"interests\", StringType(), nullable = True),\n",
        "          StructField(\"overview_html\", StringType(), nullable = True),\n",
        "          StructField(\"specialties\", StringType(), nullable = True),\n",
        "          StructField(\"homepage\", ArrayType(StringType()), nullable = True),\n",
        "          StructField(\"honors\", ArrayType(StringType()), nullable = True),\n",
        "          StructField(\"url\", StringType(), nullable = True),\n",
        "          StructField(\"also_view\", ArrayType(\n",
        "              StructType([\n",
        "                  StructField(\"url\", StringType(), nullable = True),\n",
        "                  StructField(\"id\", StringType(), nullable = True)\n",
        "              ])\n",
        "          ), nullable = True),\n",
        "          StructField(\"events\", ArrayType(\\\n",
        "              StructType([\n",
        "                  StructField(\"from\", StringType(), nullable = True),\n",
        "                  StructField(\"to\", StringType(), nullable = True),\n",
        "                  StructField(\"title1\", StringType(), nullable = True),\n",
        "                  StructField(\"start\", IntegerType(), nullable = True),\n",
        "                  StructField(\"title2\", StringType(), nullable = True),\n",
        "                  StructField(\"end\", IntegerType(), nullable = True)\n",
        "              ])\n",
        "          ), nullable = True)\n",
        "          ])\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Su604X9ggc2"
      },
      "source": [
        "### Step 1.2: The Laudable Loading\n",
        "\n",
        "Load the `linkedin.json` dataset from your S3 bucket into a Spark dataframe (sdf) called `raw_data_sdf`. If you have constructed `schema` correctly `spark.read.json()` will read in the dataset. ***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "ji-KW2sAiB6r",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "raw_data_sdf = spark.read.json(\"s3a://grewal-545-emr/linkedin.json\", schema=schema)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jMVCVotcE1wv"
      },
      "source": [
        "The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. ***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "NJSVWeGiEO5c",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Create SQL-accesible table\n",
        "raw_data_sdf.createOrReplaceTempView(\"raw_data\")\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "query = '''SELECT * \n",
        "           FROM raw_data'''\n",
        "\n",
        "# Save the output sdf of spark.sql() as answer_sdf\n",
        "answer_sdf = spark.sql(query)\n",
        "\n",
        "# Display the first 10 rows\n",
        "answer_sdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "svOO4iLPist4"
      },
      "source": [
        "### Step 1.3: The Extravagent Extraction\n",
        "\n",
        "In our training model, we are interested in when individuals began working at a company.  From creating the schema, you should notice that the collection of companies inviduals worked at are contained in the `experience` field as an array of dictionaries. You should use the `org` for the company name and `start` for the start date. Here is an example of an `experience` field,\n",
        "\n",
        "```\n",
        "{\n",
        "   \"experience\": [\n",
        "     {\n",
        "        \"org\": \"The Walt Disney Company\", \n",
        "        \"title\" : \"Mickey Mouse\",\n",
        "        \"end\" : \"Present\",\n",
        "        \"start\": \"November 1928\",\n",
        "        \"desc\": \"Sailed a boat.\"\n",
        "     },\n",
        "     {\n",
        "        \"org\": \"Walt Disney World Resort\",\n",
        "        \"title\": \"Mickey Mouse Mascot\",\n",
        "        \"start\": \"January 2005\",\n",
        "        \"desc\": \"Took pictures with kids.\"\n",
        "     }\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "Your task is to extract each pair of company and start date from these arrays. In Spark, this is known as \"exploding\" a row. An explode will seperate the elements of an array into multiple rows.\n",
        "\n",
        "Create an sdf called `raw_start_dates_sdf` that contains the company and start date for every experience of every individual in `raw_data_sdf`. Drop any row that contains a `null` in either column with `dropna()`. Remember we will sort the dataframe when grading so you can sort the elements however you wish (you don't need to if you don't want to). The sdf should look as follows:\n",
        "\n",
        "```\n",
        "+--------------------------+---------------+\n",
        "|org                       |start_date     |\n",
        "+--------------------------+---------------+\n",
        "|Walt Disney World Resort  |January 2005   | \n",
        "|The Walt Disney Company   |November 1928  |\n",
        "|...                       |...            |\n",
        "+--------------------------+---------------+\n",
        "```\n",
        "\n",
        "_Hint_: You may want to do two seperate explodes for `org` and `start`. In an explode, the position of the element in the array can be extracted as well, and used to merge two seperate explodes. Reference the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html).\n",
        "\n",
        "_Note_: Some of the entires in `org` are \"weird\", i.e. made up of non-english letters and characters. Keep them. **DO NOT** edit any name in the original dataframe unless we specify. **DO NOT** drop any row unless there is a `null` value as stated before. This goes for the rest of the homework as well, unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kt16tyP0klQX",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [raw_start_dates_sdf]\n",
        "\n",
        "raw_data_sdf.createOrReplaceTempView(\"table_1\")\n",
        "query = '''SELECT INLINE(experience)\n",
        "           FROM table_1\n",
        "           '''\n",
        "table_1_sdf = spark.sql(query)\n",
        "table_1_sdf.show(10)\n",
        "\n",
        "table_1_sdf = table_1_sdf.select(table_1_sdf['org'], table_1_sdf['start']).where('org is not null and start is not null')\n",
        "table_1_sdf.createOrReplaceTempView(\"table_2\")\n",
        "query = '''SELECT org, start as start_date\n",
        "           FROM table_2'''\n",
        "\n",
        "raw_start_dates_sdf = spark.sql(query)\n",
        "raw_start_dates_sdf.show(10)\n",
        "\n",
        "# Display the first 10 rows\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zSbb0t-d-VFt"
      },
      "source": [
        "### Step 1.4: The Fortuitous Formatting\n",
        "\n",
        "There are two issues with the values in our `date` column. First, the values are saved as strings, not datetime types. This halts us from running functions such as `ORDER BY` or `GROUP BY` on common months or years. Second, some values do not have both month and year information or are in other languages. Your task is to filter out and clean the `date` column. We are interested in only those rows that have date in the following format \"(month_name) (year)\", e.g. \"October 2010\".\n",
        "\n",
        "Create an sdf called `filtered_start_dates_sdf` from `raw_start_dates_sdf` with the `date` column filtered in the manner above. Keep only those rows with a start date between January 2000 to December 2011, inclusive. Ensure that any dates that are not in our desired format are ommitted. Drop any row that contains a `null` in either column. The format of the sdf is shown below:\n",
        "```\n",
        "+--------------------------+---------------+\n",
        "|org                       |start_date     |\n",
        "+--------------------------+---------------+\n",
        "|Walt Disney World Resort  |2005-01-01     | \n",
        "|...                       |...            |\n",
        "+--------------------------+---------------+\n",
        "```\n",
        "_Hint_: Refer to the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) to format the `date` column. In Spark SQL the date format we are interested in is `\"MMM y\"`.\n",
        "\n",
        "_Note_: Spark will return the date in the format above, with the day as `01`. This is ok, since we are interested in the month and year each individual began working and all dates will have `01` as their day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eelgTtOc_MBM",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filtered_start_dates_sdf]\n",
        "\n",
        "raw_start_dates_sdf.createOrReplaceTempView(\"table_2\")\n",
        "\n",
        "query = '''SELECT org, to_date(start_date, 'MMM y') as new_date\n",
        "           FROM table_2\n",
        "           WHERE start_date IS NOT NULL\n",
        "           '''\n",
        "#WHERE start_date BETWEEN 'JANUARY 2000' AND 'DECEMBER 2011'\n",
        "temp_sdf = spark.sql(query)\n",
        "\n",
        "temp_sdf.createOrReplaceTempView(\"table_3\")\n",
        "query_2 = '''SELECT org, new_date as start_date\n",
        "             FROM table_3\n",
        "             WHERE new_date BETWEEN to_date('JANUARY 2000', 'MMM y') AND to_date('DECEMBER 2011', 'MMM y')'''\n",
        "\n",
        "filtered_start_dates_sdf = spark.sql(query_2)\n",
        "filtered_start_dates_sdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LXYYQn2_GYwZ"
      },
      "source": [
        "### Step 1.5 The Gregarious Grouping\n",
        "\n",
        "We now want to collect the number of individuals that started in the same month and year for each company. Create an sdf called `start_dates_sdf` that has the total number of employees who began working at the same company on the same start date. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+--------------------------+---------------+---------------+\n",
        "|org                       |start_date     |num_employees  |\n",
        "+--------------------------+---------------+---------------+\n",
        "|Walt Disney World Resort  |2005-01-01     |1              |\n",
        "|...                       |...            |...            |\n",
        "+--------------------------+---------------+---------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxVIyc1CHooV",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [start_dates_sdf]\n",
        "filtered_start_dates_sdf.createOrReplaceTempView(\"table_4\")\n",
        "\n",
        "query = '''SELECT org, start_date, COUNT(org) as num_employees\n",
        "           FROM table_4\n",
        "           GROUP BY org, start_date'''\n",
        "\n",
        "start_dates_sdf = spark.sql(query)\n",
        "#start_dates_sdf.show(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYScM3FwJnUz"
      },
      "source": [
        "## Step 2: Hiring Trends Analysis\n",
        "\n",
        "Now we will analyze `start_dates_sdf` to find monthly and annual hiring trends.\n",
        "\n",
        "### Step 2.1: The Marvelous Months\n",
        "\n",
        "Your task is to answer the question: \"On average, what month do most employees start working?\" Create an sdf called `monthly_hires_sdf` which contains the total number of employees that started working on a specific month, at any company and on any year. The `month` column should be of type `int`, i.e. 1-12. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+---------------+---------------+\n",
        "|month          |num_employees  |\n",
        "+---------------+---------------+\n",
        "|1              |...            |\n",
        "|2              |...            |\n",
        "|3              |...            |\n",
        "|...            |...            |\n",
        "+---------------+---------------+\n",
        "```\n",
        "\n",
        "Find the month in which the most employees start working and save it's number as an integer to the variable `most_common_month`.\n",
        "\n",
        "_Hint_: Be careful. The starts dates we have right now have both month and year. We only want the common months. See if you can find something in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) that will help you do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vLTmvD9WNQH3",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [monthly_hire_sdf] and find the most common month people were\n",
        "\n",
        "start_dates_sdf.createOrReplaceTempView('table_4')\n",
        "\n",
        "query = '''SELECT month(start_date) as month, sum(num_employees) as num_employees\n",
        "           FROM table_4\n",
        "           GROUP BY month'''\n",
        "\n",
        "\n",
        "monthly_hires_sdf = spark.sql(query)\n",
        "\n",
        "monthly_hires_sdf.createOrReplaceTempView('table_5')\n",
        "\n",
        "query_2 = '''SELECT MAX(num_employees) as num_employees\n",
        "             FROM table_5'''\n",
        "\n",
        "temp_table_sdf = spark.sql(query_2)\n",
        "temp_table_sdf.show(1)\n",
        "temp_table_sdf.createOrReplaceTempView('table_6')\n",
        "\n",
        "query_3 = '''SELECT t_1.month as month\n",
        "             FROM table_5 as t_1\n",
        "             JOIN table_6 AS t_2 ON t_1.num_employees = t_2.num_employees'''\n",
        "\n",
        "max_month_sdf = spark.sql(query_3)\n",
        "most_common_month = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CUtVHRcMUoWp"
      },
      "source": [
        "### Step 2.2: The Preposterous Percentages\n",
        "\n",
        "The next question we will answer is \"What is the percentage change in hires between 2010 and 2011 for each company?\" Create an sdf called `percentage_change_sdf` that has the percentage change between 2010 and 2011 for each company. The sdf should look as follows:\n",
        "\n",
        "```\n",
        "+---------------------------+--------------------+\n",
        "|org                        |percentage_change   |\n",
        "+---------------------------+--------------------+\n",
        "|Walt Disney World Resort   |12.3                |\n",
        "|...                        |...                 |\n",
        "+---------------------------+--------------------+\n",
        "```\n",
        "\n",
        "_Note_: A percentage change can be positive or negative depending \n",
        "on the difference between the two years.The formula for percent change is given below,\n",
        "\n",
        "$$\\text{% change} = \\frac{P_f-P_i}{P_f} \\times 100$$\n",
        "\n",
        "Here, $P_f$ is the final element (in this case the number of hires in 2011) and $P_i$ is initial element (the number of hires in 2010).\n",
        "\n",
        "_Hint_: This is a **difficult** question. I'm really sorry. We recommend using a combination of `GROUP BY` and `JOIN`. Keep in mind that operations between columns in SQL dataframes are often easier than those between rows. Come to office hours if you need help. Especially **Fridays** and Wednesdays _*cough *cough_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_AhhfLXpWq7y",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "# TODO: Create [percentage_change_sdf]\n",
        "filtered_start_dates_sdf.createOrReplaceTempView('new_table_1')\n",
        "query_1 = '''SELECT org, count(*) as num_employees_2010\n",
        "             FROM new_table_1\n",
        "             WHERE year(start_date) = 2010\n",
        "             GROUP BY org'''\n",
        "temp_1_sdf = spark.sql(query_1)\n",
        "\n",
        "filtered_start_dates_sdf.createOrReplaceTempView('new_table_2')\n",
        "query_2 = '''SELECT org, count(*) as num_employees_2011\n",
        "             FROM new_table_2\n",
        "             WHERE year(start_date) = 2011\n",
        "             GROUP BY org'''\n",
        "temp_2_sdf = spark.sql(query_2)\n",
        "\n",
        "temp_1_sdf.createOrReplaceTempView('new_table_3')\n",
        "temp_2_sdf.createOrReplaceTempView('new_table_4')\n",
        "\n",
        "query_3 = '''SELECT t_1.org, \n",
        "             ((t_2.num_employees_2011-t_1.num_employees_2010)/(t_2.num_employees_2011))*100 as percentage_change\n",
        "             FROM new_table_3 as t_1\n",
        "             JOIN new_table_4 as t_2 \n",
        "             ON t_1.org = t_2.org'''\n",
        "\n",
        "percentage_change_sdf=spark.sql(query_3)\n",
        "#percentage_change_sdf.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VZIfJGqDqKzX"
      },
      "source": [
        "## The Bountiful Break\n",
        "\n",
        "That last question was hard. And it's gonna get harder. Take a break. Sit back and relax for a minute. Listen to some music. Here's a [suggestion](https://www.youtube.com/watch?v=9Crm6xJLJgs).\n",
        "\n",
        "In the cell below fill out the boolean variable `whatd_you_think` with `True` if you liked it or `False` if you didn't. You will be graded on your response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEZW6f8lq7iW",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Listen to some music\n",
        "whatd_you_think = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QkF2RfLSXO0u"
      },
      "source": [
        "## Step 3: Formatting the Training Data\n",
        "\n",
        "\n",
        "Our overaching goal is to train a machine learning (ML) model that will use the monthly hiring trends of a company to predict a positive or negative gain in the company's stock in the first quarter of the following year. A ML model is trained on a set of observations. Each observation contains a set of features, `X`, and a label, `y`. The goal of the ML model is to create a function that takes any `X` as an input and outputs a predicted `y`. \n",
        "\n",
        "The machine learning model we will use is a [Random Forest Classifier](https://builtin.com/data-science/random-forest-algorithm). Each observation we will pass in will have 24 features (columns). These are the number of people hired from Jan to Dec and the company stock price on the last day of each month. The label will be the direction of the company's stock percentage change (positive, `1`, or negative, `-1`) in the first quarter of the following year. Each observation will correspond to a specified company's trends on a specified year. The format of our final training sdf is shown below. The first 26 columns define our observations, `X`, and the last column the label, `y`.\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |1            |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |-1           |\n",
        "|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "```\n",
        "\n",
        "_Note_: We will use the first three letters of each month in naming, i.e. `jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec`\n",
        "\n",
        "\n",
        "\n",
        "### Step 3.1: The Harmonious Hires\n",
        "\n",
        "Your first task is to create the first half of the training table, i.e. the `jan_hired` through `dec_hired` columns. This will involve reshaping `start_dates_sdf`. Currently, `start_dates_sdf` has columns `org`, `start_date`, and `num_employees`. We want to group the rows together based on common `org` and years and create new columns for the number of employees that started working in each month of that year.\n",
        "\n",
        "Create an sdf called `raw_hirings_for_training_sdf` that has for a single company and a single year, the number of hires in Jan through Dec, and the total number of hires that year. Note that for each company you will have several rows corresponding to years between 2000 and 2011. It is ok if for a given company you don't have a given year. However, ensure that for a given company and given year, each month column has an entry, i.e. if no one was hired the value should be `0`. The format of the sdf is shown below: \n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |total_num |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |\n",
        "|... |...  |...       |   ...   |...       |...       |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "```\n",
        "_Hint_: This is a **difficult difficult** question. I'm really really sorry. The tricky part is creating the additional columns of monthly hires, specifically when there are missing dates. In our dataset, if a company did not hire anybody in a given date, it will not appear in `start_dates_sdf`. We suggest you look into `CASE` and `WHEN` statements in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "btp2wboHqg2J",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [raw_hire_train_sdf]\n",
        "\n",
        "start_dates_sdf.createOrReplaceTempView(\"table_1\")\n",
        "query = '''SELECT org, year(start_date) as year, \n",
        "           MAX(CASE month(start_date) WHEN 1 THEN num_employees ELSE 0 END) AS jan_hired,\n",
        "           MAX(CASE month(start_date) WHEN 2 THEN num_employees ELSE 0 END) AS feb_hired,\n",
        "           MAX(CASE month(start_date) WHEN 3 THEN num_employees ELSE 0 END) AS mar_hired,\n",
        "           MAX(CASE month(start_date) WHEN 4 THEN num_employees ELSE 0 END) AS apr_hired,\n",
        "           MAX(CASE month(start_date) WHEN 5 THEN num_employees ELSE 0 END) AS may_hired,\n",
        "           MAX(CASE month(start_date) WHEN 6 THEN num_employees ELSE 0 END) AS jun_hired,\n",
        "           MAX(CASE month(start_date) WHEN 7 THEN num_employees ELSE 0 END) AS jul_hired,\n",
        "           MAX(CASE month(start_date) WHEN 8 THEN num_employees ELSE 0 END) AS aug_hired,\n",
        "           MAX(CASE month(start_date) WHEN 9 THEN num_employees ELSE 0 END) AS sep_hired,\n",
        "           MAX(CASE month(start_date) WHEN 10 THEN num_employees ELSE 0 END) AS oct_hired,\n",
        "           MAX(CASE month(start_date) WHEN 11 THEN num_employees ELSE 0 END) AS nov_hired,\n",
        "           MAX(CASE month(start_date) WHEN 12 THEN num_employees ELSE 0 END) AS dec_hired,\n",
        "           SUM(num_employees) as total_num\n",
        "           FROM table_1\n",
        "           GROUP BY org, year'''\n",
        "\n",
        "raw_hire_train_sdf = spark.sql(query)\n",
        "raw_hire_train_sdf.show(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IkXyet6rrczK"
      },
      "source": [
        "### Step 3.2: The Formidable Filters\n",
        "\n",
        "Create an sdf called `hire_train_sdf` that contains all the observations in `raw_hire_train_sdf` with `total_num` greater than or equal to 500. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |total_num |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |\n",
        "|... |...  |...       |   ...   |...       |...       |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dCH4mbNcshq9",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [hire_train_sdf]\n",
        "raw_hire_train_sdf.createOrReplaceTempView(\"table_new\")\n",
        "\n",
        "query = '''SELECT * \n",
        "           FROM table_new\n",
        "           WHERE total_num >= 500'''\n",
        "           \n",
        "hire_train_sdf = spark.sql(query)\n",
        "hire_train_sdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MN4ik70Hta01"
      },
      "source": [
        "### Step 3.3: The Stupendous Stocks\n",
        "\n",
        "Now we are ready for the stock data. The stock data we will use is saved in the same S3 bucket as `linkedin.json`. Load the data into the EMR cluster. Run the cell below. ***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "APv4BxKw643q",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Load stock data\n",
        "raw_stocks_sdf = spark.read.format(\"csv\") \\\n",
        "              .option(\"header\", \"true\") \\\n",
        "              .load(\"s3a://grewal-545-emr/stock_prices.csv\")\n",
        "\n",
        "# Creates SQL-accesible table\n",
        "raw_stocks_sdf.createOrReplaceTempView('raw_stocks')\n",
        "\n",
        "# Display the first 10 rows\n",
        "query = '''SELECT *\n",
        "           FROM raw_stocks'''\n",
        "spark.sql(query).show(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUCdr3zDUAFH"
      },
      "source": [
        "Run the cell below to see the types of the columns in our data frame. These are not correct. We could have defined a schema when reading in data but we will handle this issue in another manner. You will do this in Step 3.4.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "editable": false,
        "id": "oNTGEfxsisqs",
        "colab": {}
      },
      "source": [
        "%%spark \n",
        "\n",
        "# Print types of SDF\n",
        "raw_stocks_sdf.dtypes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DdnoFkP7mxz"
      },
      "source": [
        "### Step 3.4 The Clairvoyant Cleaning\n",
        "\n",
        "We now want to format the stock data set into the second half of the training table. We will then merge it with `hire_train` based off the common `org` and `year` fields. The formatting will consist of 4 steps. Actually, it is 5.\n",
        "\n",
        "#### Step 3.4.1 The Ubiquitous UDF\n",
        "\n",
        "The companies in our stock dataset are defined by their stock tickers. Thus, we would not be able to merge it with the `org` field in `hire_train_sdf`. We must convert them to that format. Often times when using Spark, there may not be a built-in SQL function that can do the operation we desired. Instead, we can create one on our own with a user-defined function (udf).\n",
        "\n",
        "A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `TICKER_TO_NAME()` that will convert the ticker field in `raw_stocks` to the company's name. This will be done using the provided `ticker_to_name_dict` dictionary. We are only interested in the companies in that dictionary.\n",
        "\n",
        "Fill out the function `ticker_to_name()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `StringType()`. Ensure that your function returns this. You must also deal with any potential `null` cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P4cJWZsr8iNC",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Fill out [ticker_to_name()] and register it as a udf.\n",
        "\n",
        "# Dictionary linking stock ticker's to their name\n",
        "ticker_to_name_dict = {'NOK': 'Nokia',\n",
        "                       'UN': 'Unilever',\n",
        "                       'BP': 'BP',\n",
        "                       'JNJ': 'Johnson & Johnson',\n",
        "                       'TCS': 'Tata Consultancy Services',\n",
        "                       'SLB': 'Schlumberger',\n",
        "                       'NVS': 'Novartis',\n",
        "                       'CNY': 'Huawei',\n",
        "                       'PFE': 'Pfizer',\n",
        "                       'ACN': 'Accenture',\n",
        "                       'DELL': 'Dell',\n",
        "                       'MS': 'Morgan Stanley',\n",
        "                       'ORCL': 'Oracle',\n",
        "                       'BAC': 'Bank of America',\n",
        "                       'PG': 'Procter & Gamble',\n",
        "                       'CGEMY': 'Capgemini',\n",
        "                       'GS': 'Goldman Sachs',\n",
        "                       'C': 'Citi',\n",
        "                       'IBM': 'IBM',\n",
        "                       'CS': 'Credit Suisse',\n",
        "                       'MDLZ': 'Kraft Foods',\n",
        "                       'WIT': 'Wipro Technologies',\n",
        "                       'CSCO': 'Cisco Systems',\n",
        "                       'PWC': 'PwC',\n",
        "                       'GOOGL': 'Google',\n",
        "                       'CTSH': 'Cognizant Technology Solutions',\n",
        "                       'HSBC': 'HSBC',\n",
        "                       'DB': 'Deutsche Bank',\n",
        "                       'MSFT': 'Microsoft',\n",
        "                       'HPE': 'Hewlett-Packard',\n",
        "                       'ERIC': 'Ericsson',\n",
        "                       'BCS': 'Barclays Capital',\n",
        "                       'GSK': 'GlaxoSmithKline'}\n",
        "\n",
        "# Fill out ticker_to_name()\n",
        "def ticker_to_name(ticker):\n",
        "    keys = ticker_to_name_dict.keys()\n",
        "    if ticker in keys:\n",
        "       return ticker_to_name_dict.get(ticker)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "# Register udf as a SQL function. DO NOT EDIT\n",
        "spark.udf.register(\"TICKER_TO_NAME\", ticker_to_name, StringType())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u9YOYO9L-_GS"
      },
      "source": [
        "#### Step 3.4.2: The Fastidious Filters\n",
        "\n",
        "With our new `TICKER_TO_NAME()` function we will begin to wrangle `raw_stocks_sdf`.\n",
        "\n",
        "Create an sdf called `filter_1_stocks_sdf` as follows. Convert all the ticker names in `raw_stocks_sdf` to the company names and save it as `org`. Next, convert the `date` field to a datetime type. As explained before this will help order and group the rows in future steps. Then, convert the type of the values in `closing_price` to `float`. This will take care of the `dtypes` issue we saw in Step 3.3.\n",
        "\n",
        "Drop any company names that do not appear in `ticker_to_name_dict`. Keep any date between January 1st 2001 and December 4th 2012 inclusive, in the format shown below (note this is a datetime object not a string):\n",
        "\n",
        "```\n",
        "+----+------------+--------------+\n",
        "|org |date        |closing_price |\n",
        "+----+------------+--------------+\n",
        "|IBM |2000-01-03  |...           |\n",
        "|... |...         |...           |\n",
        "+----+------------+--------------+\n",
        "```\n",
        "_Hint_: You will use a similar function to filter the dates as in Step 1.4. In Spark SQL the format for the `date` field in `raw_stocks_sdf` is `\"yyyy-MM-dd\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RuiitnWlBYJ7",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filter_1_stocks_sdf]\n",
        "raw_stocks_sdf.createOrReplaceTempView('table_1')\n",
        "keys = ticker_to_name_dict.keys()\n",
        "\n",
        "query = '''SELECT TICKER_TO_NAME(ticker) as org, to_date(date, 'yyyy-MM-dd') as date, float(closing_price) as closing_price\n",
        "           FROM table_1\n",
        "           WHERE TICKER_TO_NAME(ticker) IS NOT NULL'''\n",
        "\n",
        "temp_1 = spark.sql(query)\n",
        "\n",
        "temp_1.createOrReplaceTempView('table_2')\n",
        "query_2 = '''SELECT * \n",
        "             FROM table_2\n",
        "             WHERE date BETWEEN to_date('2001-01-01', 'yyyy-MM-dd') AND to_date('2012-12-04', 'yyyy-MM-dd')'''\n",
        "           \n",
        "filter_1_stocks_sdf = spark.sql(query_2)\n",
        "filter_1_stocks_sdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ne5NaT-6CLns"
      },
      "source": [
        "#### Step 3.4.3: The Magnanimous Months\n",
        "\n",
        "The data in `filter_1_stocks_sdf` gives closing prices on a daily basis. Since we are interested in monthly trends, we will only keep the closing price on the **last trading day of each month**.\n",
        "\n",
        "Create an sdf `filter_2_stocks_sdf` that contains only the closing prices for the last trading day of each month. Note that a trading day is not simply the last day of each month, as this could be on a weekend when the market is closed . The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+------------+--------------+\n",
        "|org |date        |closing_price |\n",
        "+----+------------+--------------+\n",
        "|IBM |2000-01-31  |...           |\n",
        "|... |...         |...           |\n",
        "+----+------------+--------------+\n",
        "```\n",
        "\n",
        "  _Hint_: This is a **difficult** question. But if you made it this far, you're a star by now. It may be helpful to create an intermediate dataframe that will help you filter out the specific dates you desire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIx5LUuDD4q_",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filter_2_stocks_sdf]\n",
        "filter_1_stocks_sdf.createOrReplaceTempView('table_1')\n",
        "query = '''SELECT org, MAX(date) as date\n",
        "           FROM table_1\n",
        "           GROUP BY org, year(date), month(date)\n",
        "           ORDER BY date ASC'''\n",
        "\n",
        "temp_sdf = spark.sql(query)\n",
        "\n",
        "temp_sdf.createOrReplaceTempView('t_1')\n",
        "query_1 = '''SELECT t_1.org, t_1.date, t_2.closing_price\n",
        "             FROM t_1 as t_1, table_2 as t_2\n",
        "             WHERE t_1.date = t_2.date \n",
        "             AND t_1.org = t_2.org'''\n",
        "\n",
        "filter_2_stocks_sdf = spark.sql(query_1)\n",
        "filter_2_stocks_sdf.show(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AG4bACKKEQNl"
      },
      "source": [
        "#### Step 3.4.4: The Rambunctious Reshape\n",
        "\n",
        "Now, we will begin to shape our dataframe into the format of the final training sdf.\n",
        "\n",
        "Create an sdf `filter_3_stocks_sdf` that has for a single company and a single year, the closing stock price for the last trading day of each month in that year. This is similar to the table you created in Step 3.1. In this case since we cannot make a proxy for the closing price if the data is not avaliable, drop any rows containing any `null` values, in any column. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+\n",
        "|org |year |jan_stock |   ...   |dec_stock |\n",
        "+----+-----+----------+---------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |\n",
        "|... |...  |...       |   ...   |...       |\n",
        "+----+-----+----------+---------+----------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AucLEgvwIr_0",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filter_3_stocks_sdf]\n",
        "\n",
        "filter_2_stocks_sdf.createOrReplaceTempView('table_1')\n",
        "query = '''SELECT org, year(date) as year,\n",
        "           MAX(CASE WHEN month(date) = 1 THEN closing_price ELSE NULL END) AS jan_stock,\n",
        "           MAX(CASE WHEN month(date) = 2 THEN closing_price ELSE NULL END) AS feb_stock,\n",
        "           MAX(CASE WHEN month(date) = 3 THEN closing_price ELSE NULL END) AS mar_stock,\n",
        "           MAX(CASE WHEN month(date) = 4 THEN closing_price ELSE NULL END) AS apr_stock,\n",
        "           MAX(CASE WHEN month(date) = 5 THEN closing_price ELSE NULL END) AS may_stock,\n",
        "           MAX(CASE WHEN month(date) = 6 THEN closing_price ELSE NULL END) AS jun_stock,\n",
        "           MAX(CASE WHEN month(date) = 7 THEN closing_price ELSE NULL END) AS jul_stock,\n",
        "           MAX(CASE WHEN month(date) = 8 THEN closing_price ELSE NULL END) AS aug_stock,\n",
        "           MAX(CASE WHEN month(date) = 9 THEN closing_price ELSE NULL END) AS sep_stock,\n",
        "           MAX(CASE WHEN month(date) = 10 THEN closing_price ELSE NULL END) AS oct_stock,\n",
        "           MAX(CASE WHEN month(date) = 11 THEN closing_price ELSE NULL END) AS nov_stock,\n",
        "           MAX(CASE WHEN month(date) = 12 THEN closing_price ELSE NULL END) AS dec_stock\n",
        "           FROM table_1\n",
        "           GROUP BY org, year'''\n",
        "\n",
        "filter_3_stocks_sdf = spark.sql(query).dropna()\n",
        "filter_3_stocks_sdf.show(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "82OQKp-nIulq"
      },
      "source": [
        "#### Step 3.4.5: The Decisive Direction\n",
        "\n",
        "The final element in our training set is the binary output for each case, i.e. the `y` label. \n",
        "\n",
        "Create an sdf `stocks_train_sdf` from `filter_3_stocks_sdf` with an additional column `direction`. This should be the direction of percentage change in the closing stock price, i.e. `1` for positive or `-1` for negative, in the first quarter of a given year. The quarter of a year begins in January and ends in April, inclusive. We want to know the percent change between these two months. Reference Step 2.2 for the percent change formula. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "|org |year |jan_stock |   ...   |dec_stock |direction    |\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "|IBM |2008 |...       |   ...   |...       |1.0          |\n",
        "|IBM |2009 |...       |   ...   |...       |-1.0         |\n",
        "|... |...  |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yEFJIfyZKf7B",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "filter_3_stocks_sdf.createOrReplaceTempView('table_1')\n",
        "query = '''SELECT *, \n",
        "           CASE WHEN (((t_1.apr_stock-t_1.jan_stock)/(t_1.apr_stock))*100) >0 THEN 1 ELSE -1 END AS direction\n",
        "           FROM table_1 as t_1'''\n",
        "\n",
        "stocks_train_sdf = spark.sql(query)\n",
        "stocks_train_sdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fd2nviNpM2dF"
      },
      "source": [
        "### Step 3.5: The Capricious Combination\n",
        "\n",
        "Now that we have individually created the two halfs of our training data we will merge them together to create the final training sdf we showed in the beginning of Step 3.\n",
        "\n",
        "Create an sdf called `training_sdf` in the format of the one shown at the beginning of Step 3. Note that in our definition for the `stock_result` column, the `stock_result` value for a particular year corresponds to the direction of the stock percentage change in the **following** year. For example, the stock_result in the `2008` row for `IBM` will contain the direction of IBM's stock in the first quarter of 2009. The format of the sdf is shown below:\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |-1.0         |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |1.0          |\n",
        "|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ZIb6QkcO5RB",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [training_sdf]\n",
        "\n",
        "hire_train_sdf.createOrReplaceTempView('table_1')\n",
        "stocks_train_sdf.createOrReplaceTempView('table_2')\n",
        "\n",
        "query = '''SELECT t_1.org, t_1.year, t_1.jan_hired, t_1.feb_hired, t_1.mar_hired,t_1.apr_hired, t_1.may_hired,\n",
        "           t_1.jun_hired, t_1.jul_hired, t_1.aug_hired, t_1.sep_hired, t_1.oct_hired, t_1.nov_hired, t_1.dec_hired,t_2.jan_stock, t_2.feb_stock, \n",
        "           t_2.mar_stock, t_2.apr_stock, t_2.may_stock, t_2.jun_stock, t_2.jul_stock, t_2.aug_stock, t_2.sep_stock, t_2.oct_stock, t_2.nov_stock, t_2.dec_stock\n",
        "           FROM table_1 as t_1, table_2 as t_2\n",
        "           WHERE t_1.org = t_2.org AND t_1.year = t_2.year'''\n",
        "\n",
        "temp_sdf = spark.sql(query)\n",
        "temp_sdf.createOrReplaceTempView('table_3')\n",
        "\n",
        "query_2 \n",
        "query_1 = '''SELECT t_3.*, t_2.direction as stock_result\n",
        "             FROM table_3 as t_3, table_2 as t_2\n",
        "             WHERE t_3.org = t_2.org AND t_3.year + 1 = t_2.year'''\n",
        "training_sdf = spark.sql(query_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZnwHS5-kPtVl"
      },
      "source": [
        "## Step 4: Running the Model\n",
        "\n",
        "Well here we go. Who's ready to make some money? Well... it's not gonna happen. We didn't code the random forest model, sorry blame ESE 530. The remainder of the course will be about machine learning and we will learn how to take this beautiful data and make billions of dollars, here is some cool ASCII for the meantime.\n",
        "\n",
        "One last thing, as I predicted before, you're a star.\n",
        "\n",
        "```\n",
        "\n",
        "``````````````````````````````````....-:/+ooooo++/:-..``````````````................://-` ``````````\n",
        "``````````````````````````````.-/syhhhhhdhhhhhdhhhhhyssso+/-.```````................://-` ``````````\n",
        "`````````````````````````````.+syhhhhdddddmmddhdddhyyyhhhhhhyo+:.```................://-` ``````````\n",
        "`````````````````````..-:/+++osyhddhddddmmmmdddddddhhhyhddhdddhhhs:.................://-` ``````````\n",
        "``````````````````.-/osssyyyyyyyhyyyhdddmmmmmmmmddmhhdddyhddddmdhddyo:......`.......://.` ``````````\n",
        "`````````````````-/oyyhhdmmmdddddddddhhhdddmmdmmmdmmmdddddhhddhhddhdmy+-....`.......://-` ``````````\n",
        "```````````````./syhhhddmmmmmNNNmNmmmmmmmddddddddmmmmNmdmmddddmdhhddhdds:...........://-` ``````````\n",
        "`````````````./syyyyhhhdddmmmmmNmmmmmmmmmmmdddddhddmddmmmmmmmdddmdhhddyhh+-.........://.` ``````````\n",
        "````````````-oyysyyyhhhhhhhdddmmmdddddmmmmmddddmdddddddmmmmmmmmmdddhyddhhdy:.......-///.` ``````````\n",
        "```````````/yyssyhhhdddhyyssyyyyyyyyyyyhhhhdddddmddddhydmmmmmmmmmmhhyhhhdhyy:.......///.` ``````````\n",
        "`````````./yyssyydddddhysooooooooooossssssyyhhhhddmdddmhhmmmmmmmddmdhhhhhddhy-......///.````````````\n",
        "````````./yhssyhddmmdyso+++++++oooooooooosssyhhhhdddddmmdmmNNNmmmmddmddhhhhdho......///-````````````\n",
        "````````.shysydhmdNmyoo+++++++++++++++ooooossyyhhhhhddmmmmdNNNNNmmmdhdmdhhhhyy:....-///-````````````\n",
        "````````.ohyyhddmmmyoo++++++++++++++++++ooossssyyyyyhhdmddmmmNNNNmdmdhhddhhhyy:....-///-````````````\n",
        "```````..+yyyddmmmdso+++++///+++++++++++oooosssssyyyyyhdmmdmmmmmmNmddhyhdhyysy+....-///-````````````\n",
        "````````.:oyyhhmmmho++//////+/++++++++++ooooossssssyyyhhddmmmNNmmmmmmdhhhddyhss....-///-````````````\n",
        "..```````./syyhdmmyo++/////++/+++++++++++oooossssyyhhyhhhhddmNNNNNmmmddmdhmhyss-...-///-````````````\n",
        "`..``````..+yyhddmy+++/////////+/////++++oosyyhhddmmmmddhhhhddmNNNNmdhhmmdmhsso-...-///-````````````\n",
        "......`````-+hydddsooo+++++++///////++osyhmmmmNNNNNNNmmmmdhhhddmNNNNmhhdmmdhss+-...-///.````````````\n",
        "......`````..:syddhyhhhdddhyyso+++++ossyddmddhhyyyhhhhhhddhhhhhdmNNNmhydmmdhyo+-...-//:.````````````\n",
        "......`````.`.:ohddddddhhhhhyysoooooosyyhyyyssssyyyyyyyhhhhhyyyyhdmmmdhdmNdhhs+-...-///.````````````\n",
        "``````````````./yhyyoo++oossyssso++oosyhyysyhhhhddmdddhyhdhsyyssyhdddddddddddy+:...-///-````````````\n",
        "`````````````.:++/oo++osyhhhhyyyysosydddhhhddyshdmmddddhdddhhyssssyyhhhhhhhhdhs:...-///-.```````````\n",
        "````````````.-++::/+shyoddmmhhyhyo+oshmmmhhhhyyyyssyyyyyyyhyyyyssoosyyyhhhyyyhs:...-///-.```````````\n",
        "``````......``./:://oooossyyyosso++osyhddhsssssssssssssssyyyyyysoo+oosyhddhysss:...-///:.``....`````\n",
        "``````..........:://+++oooooo+oo++oosyhhhhyssoooooooooossssssyysooo+osyyhhhyyss+...-///:.``......`..\n",
        "`````````........-/+//++++++++++++oosyyyyhhyyssoooo+oooossssssssooooosyyyhyyyyyo-..-///:.``.........\n",
        "````````````````..://///++++++//+++osyyysysyhhys+++++++oosssssssssooosyyhhyssyh+...-///:.``.........\n",
        "`````````````````.///////++++osssosyhddmmhsyhdhyso++++ooossssssssssoosyyyyssshd:...-///-.``.........\n",
        "`````````````````.///////++++sysssossyyyyyyyhyyyssoooooosssyyyyysssoossyysosdNh:...-///-.```....````\n",
        "````````..````````://///+++/+++ooo+oooosssyyyyyyyyssooosssyyyyyysssoossyyyhNNms:...-///-````````````\n",
        "```````....```````-//+//+++++++++++++++ooossyyyyhhhyyssssyyyyyyyysssossyhmMMNmo-...-///.````````````\n",
        "```````....```````./++//+++++++++++++++oossssssyhhhhyyyyssyyyssyyssssosyymMNNm+....-///-````````````\n",
        "````````...````````-///+++++++oooooosssyyyyhyhhhdhdhhyyyssyyyssyyyyssssssdMMNm+....-///-````````````\n",
        "```   `````````````.////++ooyhy++////+//oooyosdydmmhysoooyyhsosyyyyyssssshMMNm+....-///-````````````\n",
        "```    `````````````-////+oshmyosso++ooosyhdhhmdddhsoooosyyyosyyyhyyyysssyNMMm/....-///-````````````\n",
        "````````````````````.:////+++osooooooosssyyhhhhhyyysosssyhyssyhhhhhyyyyysymdh/.....-///-.````...````\n",
        "`.```````````````````.////+++//++++ooossssssssssyyssssyyhyssyhhhhhhhhyyyso:-.......-///-..``.`.`````\n",
        "`.````````````````````./++++++//+++ooooooosssssyyssssyhhhyyhdddhhdhhhhyys+:----....-///-..``...`````\n",
        "..`````````````````````./++//++++++++oooooosssysssssyyhhhhddddddddddhhyyssmmmmd/...-///-.```..``````\n",
        "..````````````````````...:++/////++++++++ooosssssssyhhhhhdddddddddddhhyyyydNNmyo:..-///-.```..``````\n",
        "..````````````````````.....:///////+++++ooossssssyhhhhhddddddmdddddhhhhhhddhhyso+-.-///-..``..``````\n",
        "`.``````` `````````````...``./+/+++++ooooosssyyyhdddddddddmmmdddhhyyhhhddhyyyysso/--///:..``....````\n",
        "`.``````` `````````````..`````-+oooosssssyyyhhdddddddmmmmmmddhhhyyyhhhhyyyyyyyssoo/-/+/:..``.....```\n",
        "..``````  `````````````````````.-+yyyhhhhhhhddddddmmmmdddddhhyyyyyyyyyyyyyyyyysssoo+/oo:..``.....```\n",
        "..````     ```````````````````````./syhhhhhdddddmmddddddhhyyysyyyysssssssssysysssssyhdmy:.``.....```\n",
        ".````       ```````````````````````.+osyyhhhhhhhhhhhyyyyyssssssssssssssssssssssssydmNNNmmhs:....````\n",
        ".`````      ``````````````````````-+y+oooossssssyyyssyyyssosssssssossssssssssssyydMNNNNNmmddy/-.````\n",
        "....```    `````````````````````.:osss++++oooossssssyyyssoooooooooossssssoosssyyhNNNNNNNNNNNmdhs:```\n",
        "```\n",
        "\n",
        "```\n",
        "                                             ``.--::::::-`                                          \n",
        "                                       `.-:/+syyhyyyyssssso:.--.`                                   \n",
        "                                   .:/+++osyhhhhhyyyyyyyyyyyyyyyyo/-`                               \n",
        "                                `/++//+osyhhdhhhhhhhhhhhhhddhsoosyhhy/`                             \n",
        "                              .+ss+//+syhhhhddddddddddhhhhhhsooosyyhhhy:                            \n",
        "                            .+yhs++/+osyyyhhhhddhhhyyysysyssooossyyyhdhhs:``                        \n",
        "                          .+syhyo+/+ossosssshdhyoo+/////+++++ossyyhhhhddhhs:``                      \n",
        "                        `:ooshhso++ossooooosyyo/::--------::/+osyhhhddddddhhs-`                     \n",
        "                       ./++ohhys++ooss++oooso/:---..--------::/osyhhdhhdddddhy/`                    \n",
        "                     `/+///syys++osyoooo+oo+:-.......-------:::/+oyhdddddhhhhhs/`                   \n",
        "                   `:so++++ssssoossooss+o+:-..........-------:::/++shddddddhhhys/`                  \n",
        "                 `.+ssoooo+ooosso+osys++/-...`.......-------::::///+shhdmdddhyyyy:                  \n",
        "              ``.:+ooossyso+oooooosss+/-..............------::::://+syhhdmmdhhhyyo`                 \n",
        "              `:+oosssyyss+/o+ossooo+:-...`...........-------:::://+oyyhhdddhyyyyo`                 \n",
        "              `:oosssyysyo//+oosooo+/.................-------:::://+oyyyhhdhddyoss-                 \n",
        "             `.+ooossssss+++ooooso+/:.................------::::///+oyyyyhhhydhsss/``               \n",
        "            ` :oo++++osoo/++ooooo++/-.................-------:::///+osyyyyhhohdysso:-.`` ````       \n",
        "             `+osso++++++/+++oos+//:-..............----:::/+ooooossssyyyyyhhyddhyso+oo++////:-`     \n",
        "            `/osyyyo++++///++sso+/+/://+oooo+/:------:/+syyhhyssssssyyhhhhhdmmdhhyoooyhhysoooo:`    \n",
        "          `.:+ssyyyyssso+//++sso+++:///+oosoo+/:-----/shhhyo+/++osyhyyhhhhhdmmmhhysoosddhyso+/:.`   \n",
        "         `.-:ossyyhhyyyyssoooyssooo+//++ssoo+++/-..-:ohhs++++shhhyyhhyyyyhhhddmdhyyssshddhys/-.`    \n",
        "      ....-:+oossyhhyyhhhyyysyyyyyyso+yyho/+/:---...:syo///::/+oooshhysooyhhddmdhyyyyyydddy+/:.     \n",
        "     `.::::/+++ossssyyhhhhyhyhhhyys+:-:::///:----...-+so+////+++oossso+/+yhddmmmmddyyyhhdhy/--`     \n",
        "      -::/+++ooo++ooosyyyhhhhhhhys+/::://::--....-..-/so//::::://///////+yhdmmmmdddhyyyhhys:`-`     \n",
        "      ./++++oo++++++++ossyyyyhhys+/:-...............-:oso/::::-----:://+syhdmmmddhhdyyyyyso```      \n",
        "     .::/+++oosssssoo++++ossyyyo/-..................--+sso/::-------://oyhhdmmdddddddyyyyo-`        \n",
        "    `/.:+oooosooosyyyyysssooss+:--..`.........--.....-/osos+/:------:/+syhddmmmddmmmddyyo+`         \n",
        "    `/-/+ossssso+osyhhhhhyyhdy/:---.......----.--....-/sysys++/:::::/+oyhdddmmmmmddddhhs/.          \n",
        "     `  `:oyhyso+osyyyhhhhhddy/:-------://:---:://--:/yddddyo++//++++syddhddmmmmmmmmddh+            \n",
        "          .ss+ooosyyhhyhhhhhhs/::::-:/++:------/+++oshdmdhysoooooo/+oyhhhhhmNmmmmmmmdh+`            \n",
        "          `/o-+oosyyyhhhhddddo/::::::++::--------::+osssssosssyyo+:/syyyhhdmmmmmddhyy/              \n",
        "           `:+:-:/+syyyhhddddy/:--::-///:+oo+///::://+osyyhhdhs+/--ohssyhhmmmmmmddhs-               \n",
        "            `.:/:-:oyhhhdddmmd+/:--::---..-://///:://///+ossso+/::oysoshhdddmmddhhs`                \n",
        "              ``  -yhhhhddddddh+/:---:--...---::::::://+osssso+//oyysyhhdmhyhsohs.                  \n",
        "                  .oysyyhhhddddh+/::-----...-----:::///++ooo++/+yhyyhdhhmmmmmdhy:                   \n",
        "                  `.:+oossssyhdhh+//:::---...----::::////+++//oyhhhhdhhh++//:-`                     \n",
        "                    ``--.`..-:+/s+++/::::--....-------::///:/+shhhhhhddhh/``                        \n",
        "                      ````````--://+++/:::--........---:::-:+syddhhddddh/-`                         \n",
        "                      `````````..:::/+o+/:::---..---:-::://+sydddddmdddh.                           \n",
        "                       ``````````:::::/+o+//:::::::::://++oyhddddmmmdddh.                           \n",
        "                      ```````````::---::/+oo+//////+++oosyhddddmmmmmdddh.                           \n",
        "                      ``````````-::-----::/+oossyyyyyyyhhdddmmmmmmdddddh.                           \n",
        "                     .:+ssooo/::::--------:://++osyhhhddmmmmmmmmddddddhh:`                          \n",
        "                    -ooydhyo/:::::---------::::://++ossyyhhhddddddddhhhhhdy/`                       \n",
        "                  `-oooyy+::----:----------:::::///++++osyyhhhddddhhhhhhhmmmdo.                     \n",
        "                `-+ooooo+--------------------::://///++osyyyhhhdhhhhhhhhhddmmdh/`                   \n",
        "             `-/+ooooooo+-.....----------------:::///++ossyyyyhhhhhhhhhhhddhdddhs:.                 \n",
        "          `-/oooooooooooo:.....----------------:::::://++ooosyhhhyyhhhhhhdhhyhhhyyys:`              \n",
        "       `-/+oooooooooooooo+-.......--------------::::::///++osyhhyyhhhhhhhhhhysyyyyyhhy/. \n",
        "\n",
        "```"
      ]
    }
  ]
}